{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cvit_workshop_task_knn_images.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmay72QUnnlq",
        "colab_type": "text"
      },
      "source": [
        "# KNN on CIFER-10\n",
        "**Task**  \n",
        "* Perform KNN classification on CIFAR-10 dataset.  \n",
        "\n",
        "**Result**  \n",
        "* Accuracy of 24% was acheived. This is better than random which would give about 10% accuracy.\n",
        "* Model was tested on 100 images due long time of execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0H_iRLZ51C7",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcovuf2PneUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ymdO-cMyNjN",
        "colab_type": "text"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ8yg8F6yMuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle(filename):\n",
        "    import pickle\n",
        "    with open(filename, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A60KACfWygMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def give_test_set():\n",
        "  test_set = unpickle('test_batch')\n",
        "  return test_set[b'data'], test_set[b'labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VSEeVfpztco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def give_train_set():\n",
        "  for i in range(1, 6):\n",
        "    filename = 'data_batch_' + str(i)\n",
        "    train_set = unpickle(filename)\n",
        "    if i == 1:\n",
        "      train_data = train_set[b'data']\n",
        "      train_labels = train_set[b'labels']\n",
        "    else:\n",
        "      train_data = np.vstack((train_data, train_set[b'data']))\n",
        "      train_labels = np.concatenate((train_labels, train_set[b'labels']))\n",
        "  return train_data, train_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPJjXvVAyRlk",
        "colab_type": "text"
      },
      "source": [
        "## KNN algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcr4br_No5UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dist(train_features, given_feature):\n",
        "    distances = np.sum(np.square(train_features - given_feature), axis=1)\n",
        "    return distances\n",
        "\n",
        "\n",
        "def kNN(k, train_features, train_labels, given_feature):\n",
        "    distances = []\n",
        "    n = train_features.shape[0]    \n",
        "    given_feature = np.tile(given_feature, (n, 1))\n",
        "\n",
        "    # Compute distance\n",
        "    distances = dist(train_features, given_feature)\n",
        "    sort_neighbors = np.argsort(distances)\n",
        "    return np.concatenate((distances[sort_neighbors][:k].reshape(-1, 1), train_labels[sort_neighbors][:k].reshape(-1, 1)), axis = 1)\n",
        "\n",
        "\n",
        "def kNN_classify(k, train_features, train_labels, given_feature):\n",
        "    tally = collections.Counter()\n",
        "    tally.update(str(int(nn[1])) for nn in kNN(k, train_features, train_labels, given_feature))\n",
        "    return int(tally.most_common(1)[0][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FROV-hGkyVMx",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUqWp7VQ4wmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data, test_labels = give_test_set()\n",
        "train_data, train_labels = give_train_set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg53-HpIpUrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45fbd49e-53dc-45c5-ae8f-f6f777bee597"
      },
      "source": [
        "k = 5\n",
        "ntest = 100  # number of samples to test on\n",
        "accuracy = 0\n",
        "for i, given_feature in enumerate(test_data):\n",
        "    predicted_class = kNN_classify(k, train_data, train_labels, given_feature)\n",
        "    if predicted_class == int(test_labels[i]):\n",
        "        accuracy += 1\n",
        "    # print(\"Progress:\", round((i+1)/len(test_data), 4), ', Accuracy:', round(accuracy/(i+1), 2))\n",
        "    if i == ntest-1:\n",
        "        final_accuracy = (accuracy / ntest) * 100\n",
        "        break\n",
        "# final_accuracy = (accuracy / len(test_data))\n",
        "print('percent accuracy =', final_accuracy)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percent accuracy = 24.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}